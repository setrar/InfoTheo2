{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597368ac-24e9-41e7-a8d3-f46cea4f673a",
   "metadata": {},
   "source": [
    "| Practice Exercises for Information-Theory 2 |\n",
    "|:-:|\n",
    "| Professor Petros Elia, elia@eurecom.fr | \n",
    "| February 9th, 2024 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140a845-e72a-497b-a3e5-baf4a8f7da30",
   "metadata": {},
   "source": [
    "#### 1) Let $\\mathbb{F}_n^2$ be the set of all binary vectors, and let $\\mathcal{A_{\\epsilon}^n}$ be the set of typical sequences based on non-trivial distribution. Which is true below?\n",
    "- a) $Prob(\\mathcal{A_{\\epsilon}^n}) \\approx 1$ or b) $Prob(\\mathcal{A_{\\epsilon}^n}) \\ll 1$\n",
    "- c) $| \\mathcal{A_{\\epsilon}^n} | \\ll 2^n$ or d) $|\\mathcal{A_{\\epsilon}^n}| \\approx 1$ ?\n",
    "\n",
    "In the above, choose a) or b) and then choose c) or d). Justify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8541f-4310-4fb9-99b3-fc13946d5a9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For this problem, we are dealing with the typical set $\\mathcal{A_{\\epsilon}^n}$ from information theory, specifically from the context of the **Asymptotic Equipartition Property (AEP)**.\n",
    "\n",
    "### Part 1: Probability of the typical set\n",
    "The typical set $\\mathcal{A_{\\epsilon}^n}$ is defined such that most of the probability mass is concentrated there as $n$ becomes large. Based on the AEP, for large $n$, the probability of drawing a sequence from the typical set approaches 1:\n",
    "\n",
    "$\n",
    "P(\\mathcal{A_{\\epsilon}^n}) \\approx 1\n",
    "$\n",
    "\n",
    "This corresponds to option **a)**. Therefore, **a)** is correct.\n",
    "\n",
    "### Part 2: Size of the typical set\n",
    "Now, let's consider the size of the typical set. For a random variable with entropy $H(X)$, the number of typical sequences is roughly $2^{nH(X)}$. Since $H(X) < \\log_2(2^n) = n$ (entropy is always less than or equal to $n$ for a binary source), we conclude that the size of the typical set is much smaller than the total number of sequences in $\\mathbb{F}_n^2$, which is $2^n$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\n",
    "|\\mathcal{A_{\\epsilon}^n}| \\ll 2^n\n",
    "$\n",
    "\n",
    "This corresponds to option **c)**.\n",
    "\n",
    "### Conclusion:\n",
    "The correct answers are:\n",
    "- **a)**: $ P(\\mathcal{A_{\\epsilon}^n}) \\approx 1 $\n",
    "- **c)**: $ |\\mathcal{A_{\\epsilon}^n}| \\ll 2^n $ \n",
    "\n",
    "These are consistent with the properties of the typical set in information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcd3f6-f4c2-4828-b342-c8686fad4d5f",
   "metadata": {},
   "source": [
    "#### 3) In a communication setting where $X$ defines the input and $Y$ defines the output, what is the connection between the mutual information $I (X ; Y )$ and the channel capacity? Offer some intuition as well as review the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca891b-f2eb-4168-a880-8f3e30f9e237",
   "metadata": {},
   "source": [
    "---\n",
    "### Connection Between Mutual Information $ I(X; Y) $ and Channel Capacity $ C $\n",
    "\n",
    "1. **Definitions**:\n",
    "   - **Mutual Information**: $ I(X; Y) = H(X) - H(X | Y) $\n",
    "     - Measures the amount of information that $ Y $ reveals about $ X $.\n",
    "   - **Channel Capacity**: $ C = \\max_{p(x)} I(X; Y) $\n",
    "     - The maximum rate at which information can be reliably transmitted over a channel.\n",
    "\n",
    "2. **Intuition**:\n",
    "   - $ I(X; Y) $ quantifies the reduction in uncertainty about $ X $ when $ Y $ is known.\n",
    "   - $ C $ is achieved by selecting the input distribution $ p(x) $ that maximizes $ I(X; Y) $.\n",
    "\n",
    "3. **Key Steps in the Proof**:\n",
    "   - **Channel Model**: For a discrete memoryless channel, characterized by transition probabilities $ P(y|x) $.\n",
    "   - **Maximization**: Find $ C $ by maximizing $ I(X; Y) $ over all input distributions.\n",
    "   - **Achievability**: Shannon's theorem shows that $ C $ can be approached with specific coding strategies, ensuring low error rates.\n",
    "\n",
    "4. **Conclusion**:\n",
    "   - The relationship between $ I(X; Y) $ and $ C $ is crucial for designing efficient communication systems, where maximizing mutual information leads to reliable transmission rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d997b-6769-49cd-9204-526c8a99219e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
